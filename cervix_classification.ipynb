{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "#from six.moves import range\n",
    "#from scipy import ndimage\n",
    "import os\n",
    "\n",
    "import PIL.Image\n",
    "from cStringIO import StringIO\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading libraries: numpy for mathematical operation,pandas for dataset operation, PIL to load images, keras provide \n",
    "    environment for processing ( here tensorflow is in backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load images from files, resize and convert to a matrix using keras.preprocessing.image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_filenames(folders):\n",
    "    image_files = []\n",
    "    \n",
    "    for folder_tuple in folders:\n",
    "        folder = folder_tuple[0]\n",
    "        label_index = folder_tuple[1] - 1\n",
    "\n",
    "        image_filepaths = [os.path.join(folder, image_filename) for image_filename in os.listdir(folder)]\n",
    "        image_files.extend([(image_filepath, label_index) for image_filepath in image_filepaths])\n",
    "        \n",
    "    return image_files\n",
    "\n",
    "def load_image(filename, target_size):\n",
    "    try:\n",
    "        img = image.load_img(filename, target_size=target_size)\n",
    "    except IOError as e:\n",
    "        print('Could not read:', filename, ':', e, ', skipping.')\n",
    "        return None\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "    \n",
    "def load_images(image_files, target_size):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_filepath = image_file[0]\n",
    "        label_index = image_file[1]\n",
    "\n",
    "        x = load_image(image_filepath, target_size)\n",
    "        #if x == None:\n",
    "         #   continue\n",
    "            \n",
    "        x_list.append(x)\n",
    "\n",
    "        y = np.zeros((1, 3))\n",
    "        y[0, label_index] = 1\n",
    "        y_list.append(y)\n",
    "        \n",
    "    X = np.vstack(x_list)\n",
    "    y = np.vstack(y_list)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to convert image pixels from -1:1 range to 0:255 range and to display an image. normalization is done because \n",
    "        at last some weights are too large to store and takes more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_input(x):\n",
    "    \"\"\"\n",
    "    Converts image pixels from -1:1 range to 0:255 range.\n",
    "    \"\"\"\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_array(a, fmt='jpg'):\n",
    "    \"\"\"\n",
    "    Displays an image inside of Jupyter notebook.\n",
    "    \"\"\"\n",
    "    a = np.uint8(a)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to display a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(model, X, y):\n",
    "    \"\"\"\n",
    "    Displays a confusion matrix and a classification report.\n",
    "    \"\"\"\n",
    "    y_predicted = np.argmax(model.predict(X), axis=1)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "\n",
    "    print(\"Confusion matrix (rows: true, columns: predicted)\")\n",
    "    print(confusion_matrix(y_true, y_predicted))\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Classification report\")\n",
    "    print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading and pre-processing\n",
    "Load images from 'train' and 'additional' folders, shuffle and split into train and dev sets in 80/20 proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = [('train/Type_1/', 1), ('train/Type_2/', 2), ('train/Type_3/', 3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/Type_1/ 248\n",
      "train/Type_2/ 782\n",
      "train/Type_3/ 451\n"
     ]
    }
   ],
   "source": [
    "for folder_tuple in train_folders:\n",
    "    print(folder_tuple[0], len(os.listdir(folder_tuple[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=np.array([224,224])\n",
    "train_fraction = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading images from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 1481\n",
      "train_files 1184\n",
      "dev_files 297\n",
      "Could not read: train/Type_1/.DS_Store : cannot identify image file 'train/Type_1/.DS_Store' , skipping.\n",
      "Could not read: train/Type_3/.DS_Store : cannot identify image file 'train/Type_3/.DS_Store' , skipping.\n",
      "Could not read: train/Type_2/.DS_Store : cannot identify image file 'train/Type_2/.DS_Store' , skipping.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-db78a3d9945a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev_files'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e1430896dcb0>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_files, target_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "image_files = load_image_filenames(train_folders)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(image_files)\n",
    "print('files', len(image_files))\n",
    "\n",
    "train_count = int(len(image_files) * train_fraction)\n",
    "\n",
    "train_files = image_files[0:train_count]\n",
    "dev_files = image_files[train_count:]\n",
    "\n",
    "print('train_files', len(train_files))\n",
    "print('dev_files', len(dev_files))\n",
    "\n",
    "X_train, y_train = load_images(train_files, target_size=target_size)\n",
    "X_dev, y_dev = load_images(dev_files, target_size=target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note able to load images in my laptop due to RAM issue. This part is done in another pc and saved in s.npz for training part and \n",
    "d.npz for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.load('s.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184, 224, 224, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train= data['a']\n",
    "y_train = data['b']\n",
    "X_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= np.load('d.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296, 224, 224, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev= data1['a']\n",
    "y_dev= data1['b']\n",
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation is done to make the data far better by increasing the amount of data which results in less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_probability = 0.3\n",
    "dense_layer_size = 512\n",
    "batch_size = 64\n",
    "epoch_count = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout used to decrease overfitting by selecting random neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# add a fully-connected layer\n",
    "x = Dense(dense_layer_size, activation='relu')(x)\n",
    "\n",
    "# add a dropout layer for regularization\n",
    "dropout = Dropout(dropout_probability)(x)\n",
    "\n",
    "# and a logistic layer\n",
    "predictions = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "for layer in model.layers[:200]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[200:]:\n",
    "   layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transfer learning is used with the help of pretrained data imagenet. we have used it after 200th layer. We have not that much dataset to train so we have used pretrained data. after this i have used pooling layer  then dense layer then finally otput function softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "reduce_lr= keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/37 [==============================] - 328s 9s/step - loss: 0.7483 - val_loss: 4.4308\n",
      "Epoch 2/20\n",
      "37/37 [==============================] - 322s 9s/step - loss: 0.5585 - val_loss: 2.2626\n",
      "Epoch 3/20\n",
      "37/37 [==============================] - 327s 9s/step - loss: 0.4543 - val_loss: 2.3600\n",
      "Epoch 4/20\n",
      "37/37 [==============================] - 339s 9s/step - loss: 0.4203 - val_loss: 1.8367\n",
      "Epoch 5/20\n",
      "37/37 [==============================] - 333s 9s/step - loss: 0.3543 - val_loss: 1.9602\n",
      "Epoch 6/20\n",
      "37/37 [==============================] - 332s 9s/step - loss: 0.2485 - val_loss: 2.3454\n",
      "Epoch 7/20\n",
      "37/37 [==============================] - 346s 9s/step - loss: 0.3280 - val_loss: 2.1600\n",
      "Epoch 8/20\n",
      "37/37 [==============================] - 347s 9s/step - loss: 0.2627 - val_loss: 2.2919\n",
      "Epoch 9/20\n",
      "37/37 [==============================] - 324s 9s/step - loss: 0.2177 - val_loss: 2.8134\n",
      "Epoch 10/20\n",
      "37/37 [==============================] - 323s 9s/step - loss: 0.2524 - val_loss: 3.0060\n",
      "Epoch 11/20\n",
      "37/37 [==============================] - 322s 9s/step - loss: 0.1906 - val_loss: 5.9897\n",
      "Epoch 12/20\n",
      "37/37 [==============================] - 320s 9s/step - loss: 0.2055 - val_loss: 3.4642\n",
      "Epoch 13/20\n",
      "37/37 [==============================] - 321s 9s/step - loss: 0.1463 - val_loss: 2.6115\n",
      "Epoch 14/20\n",
      "37/37 [==============================] - 323s 9s/step - loss: 0.1746 - val_loss: 3.6181\n",
      "Epoch 15/20\n",
      "37/37 [==============================] - 329s 9s/step - loss: 0.1504 - val_loss: 3.1146\n",
      "Epoch 16/20\n",
      "37/37 [==============================] - 350s 9s/step - loss: 0.1511 - val_loss: 2.3516\n",
      "Epoch 17/20\n",
      "37/37 [==============================] - 381s 10s/step - loss: 0.1226 - val_loss: 3.7131\n",
      "Epoch 18/20\n",
      "37/37 [==============================] - 342s 9s/step - loss: 0.1159 - val_loss: 2.9258\n",
      "Epoch 19/20\n",
      "37/37 [==============================] - 328s 9s/step - loss: 0.1269 - val_loss: 2.4196\n",
      "Epoch 20/20\n",
      "37/37 [==============================] - 339s 9s/step - loss: 0.1503 - val_loss: 3.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5414e3a750>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0, amsgrad=False), loss='categorical_crossentropy')\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32.\n",
    "                    , epochs=epoch_count,validation_data=(X_dev, y_dev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0, amsgrad=False), loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=epoch_count, verbose=2, validation_data=(X_dev, y_dev), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data i scompiled here. validation loss is inceases means overfitting occured. It can be from different ways like complex function is trained as training error is decreasing but val error increasing. we are trying to make more simple function by changing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"models.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"models.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model  and weights are saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows: true, columns: predicted)\n",
      "[[  3  36   8]\n",
      " [  2 114  30]\n",
      " [  0  48  55]]\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.06      0.12        47\n",
      "          1       0.58      0.78      0.66       146\n",
      "          2       0.59      0.53      0.56       103\n",
      "\n",
      "avg / total       0.59      0.58      0.54       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_report(model, X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to show report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('models.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the model whose weight and model is presaved. this model can be used to classify into type1, type2 and type3 cervix cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
